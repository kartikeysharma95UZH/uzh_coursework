{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 9: Convolutional Auto-Encoder\n",
    "========================================\n",
    "\n",
    "\n",
    "Microsoft Forms Document: https://forms.office.com/r/ugv3L3jv8i\n",
    "\n",
    "The task of this assignment is to compute a valuable deep feature representation for the handwritten digits of the MNIST dataset, without making use of their labels.\n",
    "For this purpose, we implement a convolutional auto-encoder that learns a $K=10$-dimensional deep feature representation of each digit.\n",
    "This representation can then be used to reconstruct images using the decoder part.\n",
    "\n",
    "Task 1: Datasets\n",
    "----------------\n",
    "\n",
    "We will make use of the default implementations of the MNIST dataset.\n",
    "As usual, we will need the training and validation set splits of MNIST, including data loaders.\n",
    "The batches of the training set should be of size $B=32$, validation set batches should contain 100 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# training set and data loader\n",
    "train_set = ...\n",
    "train_loader = ...\n",
    "\n",
    "# validation set and data loader\n",
    "validation_set = ...\n",
    "validation_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Encoder Network\n",
    "-----------------------\n",
    "\n",
    "For the encoder network, we will rely on a similar implementation from the last exercise, which is a convolutional network with two convolutional and one fully-connected layers.\n",
    "The output of the encoder network determines the deep feature representation, which we will define to be $K=10$-dimensional.\n",
    "\n",
    "There is one main difference to the network from Assignment 8, which is the way we perform our down sampling.\n",
    "Instead of choosing a maximum pooling layer, we use a stride of 2 in our convolutions.\n",
    "The ReLU activation function should be applied after each convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder (torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K):\n",
    "    # call base class constrcutor\n",
    "    super(Encoder,self).__init__()\n",
    "    # convolutional define layers\n",
    "    self.conv1 = ...\n",
    "    self.conv2 = ...\n",
    "    # activation functions will be re-used for the different stages\n",
    "    self.act = ...\n",
    "    # define fully-connected layers\n",
    "    self.flatten = ...\n",
    "    self.fc = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    # get the deep feature representation\n",
    "    deep_feature = ...\n",
    "    return deep_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Decoder Network\n",
    "-----------------------\n",
    "\n",
    "The decoder network will obtain a deep feature representation as resulting from the encoder network.\n",
    "It will learn to undo all the steps from the encoder, in order to produce an image that is of comparable size as the original images.\n",
    "For this purpose, we require our decoder network to have:\n",
    "\n",
    "* one fully-connected layer that produces the same number of outputs as are the input of the fully-connected layer of the encoder\n",
    "* we apply the ReLU activation function\n",
    "* then, the batch must be reshaped to the same dimensionality as the output of the `conv2` layer of the encoder\n",
    "* we apply a fractionally-strided convolution using the `torch.nn.ConvTranspose2d` that uses the same parameters as the `conv2` layer of the encoder; you might need to adapt the `output_padding`\n",
    "* we apply the ReLU activation function\n",
    "* we apply a fractionally-strided convolution using the same parameters as the `conv1` layer of the encoder; `output_padding` might be required to be applied\n",
    "\n",
    "Finally, the goal is to have the output to be restricted between 0 and 1.\n",
    "Think of possible ways of doing that, and apply the way that seems most reasonable.\n",
    "\n",
    "Implement a network class that provides the required functionality.\n",
    "Implement both a constructor `__init__` and a `forward` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K):\n",
    "    # call base class constrcutor\n",
    "    super(Decoder,self).__init__()\n",
    "    # fully-connected layer\n",
    "    self.fc = ...\n",
    "    # convolutional layers\n",
    "    self.deconv1 = ...\n",
    "    self.deconv2 = ...\n",
    "    # activation function\n",
    "    self.act = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    # reconstruct the output image\n",
    "    output = ...\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Joint Auto-Encoder Network\n",
    "-----------------------------------\n",
    "\n",
    "Implement an auto-encoder network that includes bot the encoder and the decoder.\n",
    "Implement both a constructor `__init__` and a `forward` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(torch.nn.Module):\n",
    "  def __init__(self, Q1, Q2, K):\n",
    "    super(AutoEncoder,self).__init__()\n",
    "    self.encoder = ...\n",
    "    self.decoder = ...\n",
    "\n",
    "  def forward(self,x):\n",
    "    # encode input\n",
    "    deep_feature = ...\n",
    "    # decode to output\n",
    "    reconstructed = ...\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Output Sizes\n",
    "--------------------\n",
    "\n",
    "Instantiate the auto-encoder network with $Q_1 = 32$, $Q_2 = 32$ and $K=10$.\n",
    "Create an input $\\mathbf X$ in the size that the `AutoEncoder` network requires.\n",
    "Provide that input to the (untrained) encoder part of the auto-encoder network to extract the deep feature representation.\n",
    "Check that the deep feature is in the desired size (K=10) \n",
    "Provide the deep feature to the (untrained) decoder part of the auto-encoder network.\n",
    "Check that the output is of dimension $28\\times28$, and its values are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on cuda device?\n",
    "device = torch.device(\"cuda\")\n",
    "# create network\n",
    "network = ...\n",
    "\n",
    "# create or select a sample\n",
    "...\n",
    "\n",
    "# use encoder to encode image and check its size\n",
    "...\n",
    "\n",
    "# use decoder to generate an image and check its size and value range\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Training Loop\n",
    "---------------------\n",
    "\n",
    "To train the auto-encoder network, we will use the $L_2$ distance between the output and the input of the network as a loss function.\n",
    "This loss function is implemented in `torch.nn.MSELoss`.\n",
    "\n",
    "Since training an auto-encoder is tricky, we will make use of the Adam optimizer.\n",
    "Choose a learning rate of $\\eta=0.001$.\n",
    "\n",
    "Implement the training loop for 10 epochs.\n",
    "Compute the average training loss and validation loss and print them at the end of each epoch.\n",
    "\n",
    "Note: If the training and validation loss do not decrease during training, try to reduce the learning rate (to $\\eta=0.0005$ or even lower) and re-start the training.\n",
    "You will need to re-initialize the network, too, i.e. by re-running the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer with appropriate learning rate\n",
    "optimizer = ...\n",
    "loss = ...\n",
    "\n",
    "for epoch in range(10):\n",
    "  # evaluate average loss for training and validation set\n",
    "  train_loss = validation_loss = 0.\n",
    "\n",
    "  for x,_ in train_loader:\n",
    "    # compute network output\n",
    "    y = ...\n",
    "    # compute loss between output and input\n",
    "    J = ...\n",
    "    # perform update\n",
    "    ...\n",
    "    # accumulate loss\n",
    "    train_loss += ...\n",
    "\n",
    "\n",
    "  # compute validation loss\n",
    "  with torch.no_grad():\n",
    "    for x,t in validation_loader:\n",
    "      # compute network output\n",
    "      y = ...\n",
    "      # compute loss\n",
    "      J = ...\n",
    "      # accumulate loss\n",
    "      validation_loss += ...\n",
    "\n",
    "\n",
    "  # print average loss for training and validation\n",
    "  print(f\"\\rEpoch {epoch}; train: {train_loss/len(train_set):1.5f}, val: {validation_loss/len(validation_set):1.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Reconstruction Result\n",
    "-----------------------------\n",
    "\n",
    "Now we want to see if we can reconstruct images from their originals.\n",
    "For this purpose, we select the first batch of our validation set images that contains 100 samples.\n",
    "We forward this batch through our auto-encoder network and plot the reconstructed samples next to the original samples.\n",
    "\n",
    "We will plot all the samples into a single plot, where we have 10 rows, each of which containing 10 pairs of original and reconstructed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first validation set batch\n",
    "input, _ = next(iter(validation_loader))\n",
    "\n",
    "# compute outputs for all samples\n",
    "output = ...\n",
    "\n",
    "# plot images\n",
    "from matplotlib import pyplot\n",
    "pyplot.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "pyplot.figure(figsize = (20,10))\n",
    "for i in range(10):\n",
    "  for j in range(10):\n",
    "    pyplot.subplot(10, 20, i*20+2*j+1)\n",
    "    pyplot.imshow(...)\n",
    "    pyplot.axis(\"off\")\n",
    "    pyplot.subplot(10, 20, i*20+2*j+2)\n",
    "    pyplot.imshow(...)\n",
    "    pyplot.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Mean Vector per Class\n",
    "-----------------------------\n",
    "\n",
    "To see if the network has learned a reasonable representation for our 10 digits, we extract the mean deep feature vectors for each of the 10 classes.\n",
    "We forward all samples of our validation set through the encoder part of our trained auto-encoder network, and compute a class-wise average of the deep features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = ...\n",
    "\n",
    "with torch.no_grad():\n",
    "  # compute means\n",
    "  for x, t in validation_loader:\n",
    "    # extract deep features from encoder\n",
    "    deep_features = ...\n",
    "    # accumulate deep features for each class\n",
    "    ...\n",
    "\n",
    "# compute means\n",
    "...\n",
    "\n",
    "assert means.shape == (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8: Decode Mixtures of Classes\n",
    "----------------------------------\n",
    "\n",
    "For each pair of class indexes, we compute the average of the deep feature representations of these two classes.\n",
    "This results in a total of $10*10=100$ deep feature representations.\n",
    "\n",
    "We use the decoder part of our trained auto-encoder network to reconstruct images from the deep feature representations.\n",
    "We plot them in a grid of size $10*10$.\n",
    "Note that the diagonal represents the reconstruction of the mean deep features for all classes, while non-diagonal elements show mixtures of two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mixtures of each two classes  \n",
    "mixtures = ...\n",
    "# use network decoder to generate images\n",
    "images = ...\n",
    "\n",
    "pyplot.figure(figsize=(10,10))\n",
    "#  and plot\n",
    "for o1 in range(10):\n",
    "  for o2 in range(10):\n",
    "    pyplot.subplot(10,10,o1*10+o2+1)\n",
    "    pyplot.imshow(...)\n",
    "    pyplot.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
