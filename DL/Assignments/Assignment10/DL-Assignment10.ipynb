{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 10: Learn to Write Like Shakespeare\n",
    "==============================================\n",
    "\n",
    "\n",
    "Microsoft Forms Document: https://forms.office.com/r/xs1Xb1pe3g\n",
    "\n",
    "In this assignment we will implement a simple recurrent network with one hidden layer.\n",
    "We train this network on a medium-size poem \"The Sonnet\" written by William Shakespeare and use it for auto-completing sentences/phrases.\n",
    "\n",
    "The data that we will use is originally provided here: http://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# download the data file\n",
    "filename = \"shakespeare.txt\"\n",
    "if not os.path.exists(filename):\n",
    "  url = \"http://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/\"\n",
    "  import urllib.request\n",
    "  urllib.request.urlretrieve(url+filename, filename)\n",
    "  print (\"Downloaded datafile\", filename)\n",
    "\n",
    "# select to run everything on CUDA\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to parse the data and turn it into a representation from which we can learn.\n",
    "First, we need to count the number of unique characters to obtain the dimension $D$ of out input and output.\n",
    "Then, we need to obtain one-hot encoding vectors for each of the characters.\n",
    "Finally, we need to implement sequences and their according targets, using zero-padding where required.\n",
    "\n",
    "Task 1: Data Characteristics\n",
    "----------------------------\n",
    "\n",
    "Load all text data from the file `shakespeare.txt`.\n",
    "Count the number of unique characters contained in the poem. \n",
    "Here, we consider only lower-case characters to reduce the alphabet size.\n",
    "At the same time, we also store the complete poem in a data variable.\n",
    "\n",
    "Please make sure that you handle the newline character at the end of each line correctly and consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data from the text file\n",
    "data = ...\n",
    "\n",
    "# extract a list of all unique characters\n",
    "characters = ...\n",
    "\n",
    "D = len(characters)\n",
    "print (f\"Collected a total of {len(data)} elements of {D} unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: One-hot Encoding\n",
    "------------------------\n",
    "\n",
    "Each of the characters need to be represented by a one-hot encoding.\n",
    "Create a dictionary that provides the encoding for each unique character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = dict()\n",
    "for c in characters:\n",
    "  one_hot[c] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Sequence Coding\n",
    "-----------------------\n",
    "\n",
    "Write a function that provides the inputs and targets for a given sequence of the specified sequence length.\n",
    "The last value of the target sequence should be the character of the given index.\n",
    "If a character would be requested from outside of the data range, prepend the inputs (and the targets) with 0.\n",
    "Assure that $\\vec t^{\\{s\\}} = \\vec x^{\\{s+1\\}}$ $\\forall s<S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence(index, S):\n",
    "  # collect both input and target encodings\n",
    "  inputs, targets = [], []\n",
    "  # go through the sequence and turn characters into encodings\n",
    "  ...\n",
    "  return torch.stack(inputs), torch.stack(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Sequences\n",
    "-----------------\n",
    "\n",
    "Get a sequence for size 5 with index 2. Assure that the data and target vectors are as desired, i.e., the first elements are 0 vectors, and later one-hot encoded data is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequence\n",
    "x,t = sequence(2,5)\n",
    "\n",
    "# perform checks\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the standard data loader with a batch size of $B=256$. Theoretically, each training sample could have its own sequence length $S$. To enable batch processing, the sequence size must be the same for each element in the batch (otherwise it cannot be transformed as one large tensor). Thus, our dataset needs to have a fixed sequence size $S$.\n",
    "\n",
    "Task 4: Dataset and Data Loader\n",
    "-------------------------------\n",
    "Implement a dataset that takes parameters $N$ (size of the dataset) and $S$ (size of the sequence).\n",
    "In the `__getitem__` function, return the `sequence` (using the function of Task 3) for the sample with the given index, i.e., both the input and the target sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, S):\n",
    "    self.S = S\n",
    "    ...\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return ...\n",
    "\n",
    "  def __len__(self):\n",
    "    return ...\n",
    "\n",
    "dataset = ...\n",
    "data_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2: Data Sizes\n",
    "------------------\n",
    "\n",
    "Check that all samples in the dataset have the desired size and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,t in data_loader:\n",
    "  # check that the data and targets are as expected\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Elman Network Implementation\n",
    "------------------------------------\n",
    "\n",
    "Manually implement an Elman network using one fully-connected layer for hidden, recurrent and output units.\n",
    "\n",
    "Implement the processing of the input in the Elman network. Make sure that logit values are computed and returned for each element in the sequence. Try to use as much tensor processing as possible. Remember the shape of $X$ is $B\\times S\\times D$, and when going through the sequence, we need to process $\\vec x^{\\{s\\}}$ separately, while working on all batch elements simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanNetwork(torch.nn.Module):\n",
    "  def __init__(self, D, K):\n",
    "    super(ElmanNetwork,self).__init__()\n",
    "    self.W1 = ...\n",
    "    self.Wr = ...\n",
    "    self.W2 = ...\n",
    "    self.activation = ...\n",
    "\n",
    "  def forward(self, x):\n",
    "    # get the shape of the data\n",
    "    B, S, D = x.shape\n",
    "    # initialize the hidden vector in the desired size with 0\n",
    "    # remember to put it on the device\n",
    "    h_s = ...\n",
    "    # store all logits (we will need them in the loss function)\n",
    "    Z = torch.empty(x.shape, device=device)\n",
    "    # iterate over the sequence\n",
    "    for s in range(S):\n",
    "      # use current sequence item\n",
    "      x_s = ...\n",
    "      # compute recurrent activation\n",
    "      a_s = ...\n",
    "      # apply activation function\n",
    "      h_s = ...\n",
    "      # compute logit values\n",
    "      z = ...\n",
    "      # store logit value\n",
    "      Z[:,s] = z\n",
    "      \n",
    "    # return logits for all sequence elements\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3: Network Output\n",
    "----------------------\n",
    "\n",
    "Instantiate an Elman network with arbitrary numbers for $D$ and $K$.\n",
    "Generate training samples in a given format, forward them through the network and assure that the results are in the required dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate test network\n",
    "test_network = ...\n",
    "\n",
    "# create test input in size BxSxD\n",
    "test_input = ...\n",
    "# get the network output\n",
    "test_output = ...\n",
    "# check that the netowrk output size is as intended\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the Elman network, we will use categorical cross-entropy loss, averaged over all samples in the sequence.\n",
    "For each batch, we will use a different sequence size -- while the size inside a batch must stay the same.\n",
    "\n",
    "According to the PyTorch documentation, the `CrossEntropyLoss` handles logits and targets in shape $B\\times O\\times\\ldots$.\n",
    "In our case, logits and targets are in dimension $B\\times S\\times O$.\n",
    "Hence, we need to make sure that we re-order the indexes such that we fulfil the requirement; you might want to use the `permute` operator.\n",
    "\n",
    "WARNING: `CrossEntropyLoss` will not complain when the order for the `CrossEntropyLoss` is wrong, just the results will be wrong.\n",
    "\n",
    "\n",
    "Task 6: Training Loop\n",
    "---------------------\n",
    "Instantiate the optimizer with an appropriate learning rate $\\eta$ and the loss function.\n",
    "Implement the training loop for 10 epochs -- more epochs will further improve the results.\n",
    "Compute the average training loss per epoch.\n",
    "Possibly, at the end of each batch, overwrite the `dataset.S` with a value randomly samples from $S\\in[5,20]$.\n",
    "\n",
    "Note that 10 epochs will train for about 2 minutes, if implemented in an optimized way, on the GPU. Non-optimized training will take considerably longer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ...\n",
    "optimizer = ...\n",
    "loss = ...\n",
    "\n",
    "for epoch in range(10):\n",
    "  # create random sequence\n",
    "  train_loss = 0.\n",
    "\n",
    "  for x, t in data_loader:\n",
    "    # compute network output\n",
    "    z = ...\n",
    "    # compute loss, arrange order of logits and targets\n",
    "    J = ...\n",
    "    # compute gradient for this batch\n",
    "\n",
    "    # compute average loss\n",
    "    train_loss += ...\n",
    "    # select a new sequence length S in [5,20]\n",
    "    dataset.S = ...\n",
    "\n",
    "  # print average loss for training and validation\n",
    "  print(f\"\\rEpoch {epoch+1}; train loss: {train_loss/len(data_loader):1.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Text Encoding\n",
    "---------------------\n",
    "For a given text (a sequence of $S$ characters), provide the encoding $\\mathcal X \\in R^{B\\times S\\times D}$.\n",
    "Assure that the batch index $B=1$ is added to the encoding, so that the network is able to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "  encoding = ...\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8: Next Element Prediction\n",
    "-------------------------------\n",
    "\n",
    "Implement a function that return the next character from the logits returned by the network.\n",
    "Note that the logits are in dimension $\\mathcal Y \\in \\mathbb R^{B\\times S\\times D}$ with $B=1$, and we are generally only interested in the prediction for the last sequence item.\n",
    "\n",
    "Select the character with the highest SoftMax probability $\\max_o z^{\\{S\\}}_o$ and append this character to the `text`.\n",
    "Alternatively, we can also randomly draw a character based on the SoftMax probability distribution $\\vec y^{\\{S\\}}$. `random.choices` provides the possibility to pass a list of characters and a list of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(z, use_best):\n",
    "  # select the appropriate logits\n",
    "  z_S = ...\n",
    "  if use_best:\n",
    "    # take character with maximum probability\n",
    "    next_char = ...\n",
    "  else:\n",
    "    # sample character based on class probabilities\n",
    "    next_char = ...\n",
    "  return next_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 9: Sequence Completion\n",
    "---------------------------\n",
    "\n",
    "Write a function that takes a `seed` text which it will complete with the given number of characters.\n",
    "Write a loop that turns the current `text` into an encoded sequence of its characters using the function from Task 7.\n",
    "Forward the text through the network and take the prediction of the last sequence item $\\vec z^{\\{S\\}}$ using the function from Task 8.\n",
    "Append this to the current text (remember that Python strings are immutable).\n",
    "Repeat this loop 80 times, and return the resulting `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_completion(seed, count, use_best):\n",
    "  # we start with the given seed\n",
    "  text = seed\n",
    "  for i in range(count):\n",
    "    # turn current text to one-hot batch\n",
    "    x = ...\n",
    "    # predict the next character\n",
    "    next_char = ...\n",
    "    # append character to text\n",
    "    ...\n",
    "    \n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 10: Text Production\n",
    "-----------------------\n",
    "\n",
    "Select several seeds (such as `\"the \", \"beau\", \"mothe\", \"bloo\"`) and let the network predict the following 80 most probable characters, or using probability sampling.\n",
    "Write the completed sentences to console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = ...\n",
    "\n",
    "for seed in seeds:\n",
    "  best = ...\n",
    "  # print seed and text\n",
    "  print (f\"\\\"{seed}\\\" -> \\\"{best}\\\"\")\n",
    "  sampled = ...\n",
    "  # print seed and text\n",
    "  print (f\"\\\"{seed}\\\" -> \\\"{sampled}\\\"\")\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
